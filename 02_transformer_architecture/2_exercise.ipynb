{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# A deeper look into generation with transformers\n",
    "\n",
    "In the last exercise, we worked with open source LLMs to generate text. Today, we will learn how to do so more efficiently.\n",
    "\n",
    "Among other things, we will be working with GPUs that can help us speed up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "from transformers import set_seed\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "seed = 1122\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4268a",
   "metadata": {},
   "source": [
    "# Exercise 1: Setting up the model\n",
    "\n",
    "Let us set up our model which we will use for the rest of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee89f5c",
   "metadata": {},
   "source": [
    "## Exercise 1a: Setting up the virtual machine with a GPU\n",
    "\n",
    "Your first task is to form groups of 3. The number of GPUs we have is capped at 10 so we cannot have more than 10 teams working simultaneously.\n",
    "\n",
    "Next connect to the GPU server by following the instructions below. Only one person from each team should connect.\n",
    "\n",
    "1. Go to https://jupyterhub.uni-muenster.de/.\n",
    "2. Log in using your RUB credentials.\n",
    "3. Launch a new virtual machine using the following config:\n",
    " * vCPU: 1-8\n",
    " * Memory: 8 - 16\n",
    " * GPU: NVIDIA A40, 12 GB RAM\n",
    "4. Use the token provided to you in the class.\n",
    "5. Wait until the jupyter lab environment opens up.\n",
    "6. Upload this notebook to Jupyter Lab. Go to the top left corner, click file and then \"Open from path\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702541a6",
   "metadata": {},
   "source": [
    "## Exercise 1b: Watching the GPU Usage [Only if you are on a machine with a GPU]\n",
    "\n",
    "1. Open a new terminal by following File -> New -> Terminal.\n",
    "2. Click on the newly opened tab within the jupyter lab. This is not a new browser tab, but a new tab within the jupyter lab environment. \n",
    "3. In the console, execute the command `watch -n1 nvidia-smi`. The command shows you the GPU usage. Right now, you should be using 0 MB or GPU RAM. THe utilization should also be 0%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea885f",
   "metadata": {},
   "source": [
    "## Exercise 1c: Downloading the model\n",
    "\n",
    "Now let us start working with a LLM. Execute the cell below to download the model and load it in GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\" # Very small model that we used in the last class. Can be used without a GPU.\n",
    "# model_name = \"ministral/Ministral-3b-instruct\" # Use if you have access to a GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acde406",
   "metadata": {},
   "source": [
    "## Exercise 1d: Generating multiple tokens with the model\n",
    "\n",
    "You already know the code below from the previous exercise. Run it again to make sure everything still works :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec299b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145d995",
   "metadata": {},
   "source": [
    "# Exercise 2: Measuring and optimizing model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b02c2",
   "metadata": {},
   "source": [
    "## Exercise 2a: Measuring model throughput [20 mins]\n",
    "\n",
    "Before you can optimize model performance, you need to be able to measure it.\n",
    "\n",
    "Rewerite the `generate` function. It should now take two new parameters as input:\n",
    "1. `num_generations: int = 10` denotes how many outputs you want to generate for each prompt.\n",
    "2. `progress: bool = True`. If this parameter is set to `True`, it should print a nested progress bar. The first bar updates every time the model generates a complete output. The second bar updates every time a new token within the output is generated. You can use [nested tqdm bars](https://github.com/tqdm/tqdm/blob/0ed5d7f18fa3153834cbac0aa57e8092b217cc16/README.rst#nested-progress-bars), which are two nested `tqdm.trange` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb52417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3800d68",
   "metadata": {},
   "source": [
    "## Exercise 2b: Speeding things up with batching [30 mins]\n",
    "\n",
    "Deep models can benefit from batching. Convert your code into a batched one where instead of generating a single output, you generate `num_generations` outputs at once. Specifically, instead of drawing a single token from the multinomial, you would draw `num_generations` tokens now.\n",
    "\n",
    "Take 10 prompts from the BOLD dataset (you used it in the previous exercise). For each prompt, generate outputs with `num_generations=10` and `gen_len=30` with _batching_ and _without batching_. Report the time taken by each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989417ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BOLD data\n",
    "n_prompts = 10\n",
    "bold = datasets.load_dataset(\"AlexaAI/bold\")\n",
    "random.seed(11)\n",
    "prompts = []\n",
    "for prompt_list in bold[\"train\"][\"prompts\"]:\n",
    "    prompts.extend(prompt_list)\n",
    "random.shuffle(prompts)\n",
    "prompts = prompts[:n_prompts]\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26392f2",
   "metadata": {},
   "source": [
    "# Exercise 3: Using `model.generate` [25 mins]\n",
    "\n",
    "Instead of using your own generate function, now use the built-in `model.generate` function with the same values for `num_generations` and `gen_lenght` (they are named slightly differently for `model.generate`). You can use the [`GenerationConfig`](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) object to control the generation. For examples, see [here](https://huggingface.co/docs/transformers/en/llm_tutorial).\n",
    "\n",
    "1. Compare the time it takes the model to generate the outputs with your own `generate` function from the previous exercise. Are there any differences? Why?\n",
    "2. The `generate` function by default uses KV cache. Set `use_cache = False` to disable KV caching. Do you notice any difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e44ea7",
   "metadata": {},
   "source": [
    "# Exercise 4: Reproducibility [15 mins]\n",
    "\n",
    "An essential part of your LLM generation pipeline is reproducibility. You want to save all the parameters used in generation so that you can get the same measurements again.\n",
    "\n",
    "Save all the parameters, e.g., seeds, generation lenght, to file. Load these parameters again and perform the inference. Check if you get identical outputs before and after the save/load operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f285715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
