{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# Efficient inference with Tranformers\n",
    "\n",
    "In this exercise, we will be looking more closely into our LLM. We will explore strategies for reducing the size of our models and its impact on the quality of model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4268a",
   "metadata": {},
   "source": [
    "# Exercise 1: Estimating the size of our model [30 mins]\n",
    "\n",
    "Recall your a HuggingFace `model` is a simple PyTorch NN. Inspect the model by doing the following:\n",
    "\n",
    "1. Print the total number of layers.\n",
    "2. Names of all the parameter tensors inside each layer, along with their data type.\n",
    "3. The output dimensionality at each layer along with it data type.\n",
    "4. The data type of each tensor and the estimated storage requirements in MBs.\n",
    "5. The total size of the model in MBs.\n",
    "\n",
    "**Hint:** You can use `print(model)` to see all the layers and sublayers inside the model.\n",
    "\n",
    "**Hint:** Recall that all the layers and the model itself are simply `torch.nn` modules. Which means you can call the functions `model.named_parameters()` or `layer.named_parameters()` to go over the parameters or the model or the layer. See the [documentation of `named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\" # Very small model that we used in the last class. Can be used without a GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7cfe5",
   "metadata": {},
   "source": [
    "# Exercise 2: Converting the model to bfloat [10 mins]\n",
    "\n",
    "You might have noticed that the `AutoModelForCausalLM.from_pretrained` loaded the model in `torch.float32` data type. Load the model again, this time by passing the augument `torch_dtype=torch.bfloat16` to the `from_pretrained` method.\n",
    "\n",
    "Compute the size of individual layers and the total model size again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0892e",
   "metadata": {},
   "source": [
    "# Exercise 3: Capturing different aspects of model outputs [20 mins]\n",
    "\n",
    "We would like to compare the difference in the parameters and outputs of the models when we quantize them.\n",
    "\n",
    "Write a function that given a prompt, returns the following stats for the original model that is loaded in `torch.float32` precision.\n",
    "\n",
    "1. First N output tokens generated in a greedy manner.\n",
    "2. Softmax probabilities at each generation position. In other words, compute these probabilities not just for output tokens but _also for input tokens_.\n",
    "3. Weights at the Kth transformer layer where K is a parameter to the function.\n",
    "\n",
    "Make sure your function always returns these all tensors in `float32` precision.\n",
    "\n",
    "**Hint:** You can use the generate function that you write in one of the previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d707c6",
   "metadata": {},
   "source": [
    "# Exercise 4: Comparing differences in model outputs [20 mins]\n",
    "\n",
    "Use three different versions of the model:\n",
    "1. The originally loaded model in `torch.float32` format.\n",
    "2. A model that you manually cast to `torch.float16` using [`model.half()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half) function.\n",
    "3. A model that you load in `torch.bfloat16` by passing the `torch.dtype=torch.bfloat16` parameter to the pretrained model.\n",
    "\n",
    "Compare the following properties of these three models:\n",
    "1. Difference in generated tokens.\n",
    "2. 10 most likely tokens at each generation position.\n",
    "3. L2 norm of difference in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f285715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
