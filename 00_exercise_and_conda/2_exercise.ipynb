{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLM Engineering\n",
    "\n",
    "Welcome to the first exercise of our course on LLM Engineering. \n",
    "\n",
    "This notebook will introduce you to the fundamental concepts of PyTorch, neural networks, and good programming practices that will serve as a foundation for working with more complex language models. By the end of this exercise, you'll be comfortable with implementing neural networks in PyTorch, understanding key concepts like train/evaluation modes, and applying best practices for model development and evaluation.\n",
    "\n",
    "Ensure you understand each section thoroughly, as they form the basis for complex topics covered later in the course. If you are not familiar with the concepts covered here, please catch up on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch for tensor operations and building neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataclasses and typing help structure data neatly and clearly\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "# Importing libraries for data manipulation and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing libraries for data loading and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Typing and Object-Oriented Programming (OOP) in Neural Networks\n",
    "\n",
    "Clean code structure, typing, and  Object-Oriented Programming (OOP) principles enhance maintainability, readability, and reusability, especially critical for large-scale ML projects.\n",
    "In this section, we'll create a basic neural network using object-oriented programming principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network\n",
    "\n",
    "A neural network consists of layers performing linear transformations and non-linear activations. Each layer applies transformations to the input data.\n",
    "\n",
    "**Example:**  Here, we define a simple feedforward neural network with hidden layers and dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"A simple neural network with configurable architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, dropout_rate: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output classes\n",
    "            dropout_rate: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_size]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, output_size]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Make predictions using the model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class indices\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `SimpleNN` class demonstrates how to inherit from PyTorch's `nn.Module`. We then initialize layers in the `__init__` function and implement a forward pass to pass data through the network. We can also add further utility methods like `predict`to make predicitions based on the model outputs.\n",
    "\n",
    "\n",
    "This pattern of encapsulating model logic in a class will be used throughout your work with neural networks and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an instance of our model\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "# Example forward pass\n",
    "example_input = torch.randn(5, input_size) # Batch of 5 samples\n",
    "output = model(example_input) # Forward pass\n",
    "prediction = model.predict(example_input) # Prediction\n",
    "print(f\"\\nExample input shape: {example_input.shape}\")\n",
    "print(f\"Example output shape: {output.shape}\")\n",
    "print(f\"Example output:\\n{output}\")\n",
    "print(f\"Predicted classes:\\n{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a deeper neural network with multiple hidden layers\n",
    "\n",
    "For this exercise, create a new class called `DeepNN` that extends nn.Module. Your implementation should include:\n",
    "* 3 hidden layers with configurable sizes\n",
    "* ReLU activations between layers\n",
    "* Dropout for regularization\n",
    "* `__init__` and `forward` methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Your implementation here...\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Structured Outputs and Parsing\n",
    "\n",
    "When working with LLMs and other complex models, it's important to structure your outputs in a way that makes them easy to process and interpret. In this section, we'll implement structured outputs using data classes.\n",
    "Using Python's `dataclass` simplifies this process by clearly defining expected attributes and their types.\n",
    "\n",
    "We defina a class `ModelOutput` to structure the outputs of our model. This class has methods for converting outputs to different formats (dict, JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelOutput:\n",
    "    \"\"\"Data class for structured model outputs.\"\"\"\n",
    "    logits: torch.Tensor\n",
    "    probabilities: torch.Tensor\n",
    "    predictions: torch.Tensor\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert outputs to dictionary format.\"\"\"\n",
    "        return {\n",
    "            \"logits\": self.logits.tolist(),\n",
    "            \"probabilities\": self.probabilities.tolist(),\n",
    "            \"predictions\": self.predictions.tolist()\n",
    "        }\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert outputs to JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict())\n",
    "    \n",
    "    @classmethod\n",
    "    def from_logits(cls, logits: torch.Tensor) -> 'ModelOutput':\n",
    "        \"\"\"Create a ModelOutput instance from logits.\"\"\"\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        return cls(logits=logits, probabilities=probabilities, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extend our SimpleNN to use structured outputs\n",
    "class StructuredNN(SimpleNN):\n",
    "    \"\"\"Neural network with structured outputs.\"\"\"\n",
    "    \n",
    "    def forward_structured(self, x: torch.Tensor) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        Forward pass with structured output.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Structured model output\n",
    "        \"\"\"\n",
    "        logits = super().forward(x)\n",
    "        return ModelOutput.from_logits(logits)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance and test the structured outputs\n",
    "structured_model = StructuredNN(input_size, hidden_size, output_size)\n",
    "structured_output = structured_model.forward_structured(example_input)\n",
    "\n",
    "print(f\"Structured output example:\\n\")\n",
    "print(f\"Logits shape: {structured_output.logits.shape}\")\n",
    "print(f\"Probabilities shape: {structured_output.probabilities.shape}\")\n",
    "print(f\"Predictions shape: {structured_output.predictions.shape}\")\n",
    "print(f\"\\nJSON representation:\\n{structured_output.to_json()[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parsing structured output\n",
    "Implement a function called `parse_output` that takes a `ModelOutput` object and returns a human-readable string. Your function should:\n",
    "* Extract the most confident prediction\n",
    "* Include its associated probability\n",
    "* Format the information in a clear, readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function called parse_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding torch.no_grad() and train/eval modes\n",
    "\n",
    "Gradients represent how much each parameter affects the final prediction, guiding parameter updates during training.\n",
    "Managing gradients efficiently is vital for training neural networks and optimizing inference.\n",
    "\n",
    "For efficient and correct model development, it's crucial to understand PyTorch's execution modes. \n",
    "PyTorch models have two primary operating modes: training and evaluation. These modes control how certain layers behave and whether gradients are computed. \n",
    "\n",
    "- `.eval()` sets the model to evaluation mode.\n",
    "- `torch.no_grad()` disables gradient calculation, saving memory during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate different behaviors in train and eval modes\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "input_data = torch.randn(5, input_size)\n",
    "\n",
    "# Training mode (default)\n",
    "model.train()\n",
    "print(\"Training mode (model.train()):\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Forward pass with gradients\n",
    "output_train = model(input_data)\n",
    "print(f\"Output shape: {output_train.shape}\")\n",
    "print(f\"Requires gradient: {output_train.requires_grad}\")\n",
    "\n",
    "# Make another pass, observe dropout\n",
    "output_train2 = model(input_data)\n",
    "diff_train = torch.sum(torch.abs(output_train - output_train2)).item()\n",
    "print(f\"Difference between two forward passes: {diff_train:.6f} (due to dropout)\")\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "print(\"\\nEvaluation mode (model.eval()):\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Forward pass in eval mode\n",
    "output_eval = model(input_data)\n",
    "print(f\"Output shape: {output_eval.shape}\")\n",
    "print(f\"Requires gradient: {output_eval.requires_grad}\")\n",
    "\n",
    "# Make another pass, observe no dropout\n",
    "output_eval2 = model(input_data)\n",
    "diff_eval = torch.sum(torch.abs(output_eval - output_eval2)).item()\n",
    "print(f\"Difference between two forward passes: {diff_eval:.6f} (should be 0, no dropout)\")\n",
    "\n",
    "# Using torch.no_grad()\n",
    "print(\"\\nUsing torch.no_grad():\")\n",
    "print(\"-\" * 30)\n",
    "with torch.no_grad():\n",
    "    output_no_grad = model(input_data)\n",
    "    print(f\"Output shape: {output_no_grad.shape}\")\n",
    "    print(f\"Requires gradient: {output_no_grad.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout Behavior in Different Modes**\n",
    "Dropout is a regularization technique that randomly sets a portion of neurons to zero during training to prevent overfitting. However, this random behavior would be problematic during inference:\n",
    "\n",
    "* In training mode (`model.train()`): Dropout randomly deactivates neurons based on the dropout probability.Each forward pass produces slightly different results due to this randomness. This intentional noise helps the model generalize better.\n",
    "* In evaluation mode (`model.eval()`): Dropout is effectively disabled (all neurons are active). Output is deterministic (same input always produces same output). No randomness is introduced to prediction.\n",
    "\n",
    "This difference is why `model.eval()` must be called before making predictions on new data to ensure consistent results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Comparing memory usage with and without gradients\n",
    "\n",
    "Training neural networks requires computing gradients for parameter updates. This process:\n",
    "* Stores intermediate activations in memory for backpropagation\n",
    "* Creates a computational graph that tracks operations\n",
    "* Significantly increases memory usage as model size grows\n",
    "\n",
    "When performing inference (making predictions), we don't need these gradients. Using `torch.no_grad()`:\n",
    "* Disables gradient calculation\n",
    "* Reduces memory usage by not storing intermediate activations\n",
    "* Speeds up computation by not building the computational graph\n",
    "* Prevents accidental parameter updates\n",
    "\n",
    "Task: Implement a function that measures and compares memory usage when processing a large batch with and without gradient calculation. When would you use which mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that measures and compares the memory usage\n",
    "# when processing a large batch with and without gradients\n",
    "\n",
    "batch = torch.randn(10000, input_size)  # Large batch\n",
    "\n",
    "\n",
    "# Hint: You can use the torch.cuda.memory_allocated() function if using GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Classes in Python\n",
    "Using dataclasses streamlines hyperparameter management, making experiments reproducible and organized. Hyperparameters are settings chosen before training that influence model learning. Hyperparameters like learning rate, batch size, and epochs affect how the model learns and performs.\n",
    "\n",
    "Data classes provide a clean way to organize configuration and results in your machine learning projects. This section introduces Python's dataclass decorator and demonstrates how to use it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Data class for neural network training configuration.\"\"\"\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 10\n",
    "    weight_decay: float = 0.0001\n",
    "    early_stopping_patience: int = 3\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "        if self.batch_size <= 0:\n",
    "            raise ValueError(\"Batch size must be positive\")\n",
    "        if self.early_stopping_patience < 0:\n",
    "            raise ValueError(\"Patience must be non-negative\")\n",
    "            \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert config to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"early_stopping_patience\": self.early_stopping_patience,\n",
    "            \"device\": self.device\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TrainingConfig`class shows how to define default values for configuration parameters. We can validate configurations with `__post_init__`, and add a method for converting configurations to other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use a configuration\n",
    "config = TrainingConfig(learning_rate=0.01, epochs=20)\n",
    "print(f\"Training configuration:\\n{config}\")\n",
    "\n",
    "# Modify and validate configurations\n",
    "try:\n",
    "    invalid_config = TrainingConfig(learning_rate=-0.1)\n",
    "except ValueError as e:\n",
    "    print(f\"Validation works: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create an ExperimentResults data class\n",
    "Create a data class called ExperimentResults to store experimental results. Your implementation should:\n",
    "* Store model name, training time, best epoch, and metrics\n",
    "* Include methods for saving to and loading from JSON files\n",
    "* Support optional test metrics (for when test data isn't available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Create a data class called ExperimentResults that stores:\n",
    "# - model_name: str\n",
    "# - training_time: float (in seconds)\n",
    "# - best_epoch: int\n",
    "# - train_metrics: Dict (containing accuracy, loss, etc.)\n",
    "# - validation_metrics: Dict\n",
    "# - test_metrics: Optional[Dict]\n",
    "# Include methods for saving to and loading from JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced NN Concepts: Regularization and Activation Functions\n",
    "\n",
    "As models grow in complexity, understanding regularization techniques and activation functions becomes crucial for achieving good performance. This section introduces advanced neural network concepts that will improve your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNN(nn.Module):\n",
    "    \"\"\"Neural network with various regularization techniques and activation functions.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        hidden_sizes: List[int], \n",
    "        output_size: int,\n",
    "        dropout_rate: float = 0.2,\n",
    "        activation: str = \"relu\",\n",
    "        use_batch_norm: bool = True,\n",
    "        weight_constraint: Optional[float] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the advanced neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            output_size: Number of output classes\n",
    "            dropout_rate: Dropout probability\n",
    "            activation: Activation function name ('relu', 'leaky_relu', 'elu', 'gelu', etc.)\n",
    "            use_batch_norm: Whether to use batch normalization\n",
    "            weight_constraint: Maximum norm for weight constraint (L2 norm)\n",
    "        \"\"\"\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.weight_constraint = weight_constraint\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        if use_batch_norm:\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if use_batch_norm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation = F.leaky_relu\n",
    "        elif activation == \"elu\":\n",
    "            self.activation = F.elu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "            \n",
    "        # Flag for batch normalization\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Apply each layer with activation, batch norm, and dropout\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.layers[i](x)\n",
    "            if self.use_batch_norm:\n",
    "                if x.dim() == 2:  # Handle batch size of 1\n",
    "                    x = self.batch_norms[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropouts[i](x)\n",
    "            \n",
    "        # Apply weight constraints if specified\n",
    "        if self.weight_constraint is not None and self.training:\n",
    "            for layer in self.layers:\n",
    "                w = layer.weight.data\n",
    "                norm = torch.norm(w, 2, dim=1, keepdim=True)\n",
    "                desired = torch.clamp(norm, 0, self.weight_constraint)\n",
    "                w = w * (desired / (1e-7 + norm))\n",
    "                layer.weight.data = w\n",
    "        \n",
    "        # Output layer (no activation or dropout)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Create a simple test for our advanced model\n",
    "hidden_sizes = [32, 16]\n",
    "advanced_model = AdvancedNN(\n",
    "    input_size=10, \n",
    "    hidden_sizes=hidden_sizes, \n",
    "    output_size=2, \n",
    "    activation=\"gelu\",\n",
    "    weight_constraint=3.0\n",
    ")\n",
    "\n",
    "print(f\"Advanced model architecture:\\n{advanced_model}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(8, input_size)\n",
    "test_output = advanced_model(test_input)\n",
    "print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this class, we added different activation functions (ReLU, Leaky ReLU, ELU, GELU), and regularization techniques (dropout, batch normalization, weight constraints).\n",
    "\n",
    "### Exercise: Compare activation functions and regularization\n",
    "Implement a function that trains the same model architecture with different activation functions and regularization settings, then compares the results. Your function should:\n",
    "* Test at least 3 different combinations of activations and regularization\n",
    "* Report training time, convergence speed, and final accuracy\n",
    "* Visualize the differences in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Create a function that trains the same model architecture with different activation functions and regularization settings, then compares the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On: Text Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll apply the concepts from Part 1 to a practical text classification task. We'll build a sentiment analyzer for movie reviews, taking you through the entire machine learning workflow from data preprocessing to model evaluation.\n",
    "We will use a dataset from the Internet (from the model & dataset database [Huggingface](https://huggingface.co/datasets/stanfordnlp/imdb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\nLoading and preprocessing example text data...\")\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"]).sample(1000, random_state=42)\n",
    "\n",
    "reviews = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(reviews)} training samples and {len(labels)} labels.\")\n",
    "print(f\"Sample text: {reviews[0]}\")\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StanfordNLP/imdb](https://huggingface.co/datasets/stanfordnlp/imdb) is a large movie review dataset. It is used for binary sentiment classification. It provides 50,000 highly polar movie reviews (of which we randomly sample 1000). A label \"0\" corresponds to a negative sentiment, \"1\" symbolizes positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training any neural network, proper data preprocessing is essential. This section demonstrates how to prepare text data for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    \"\"\"Dataset class for text classification.\"\"\"\n",
    "    texts: List[str]\n",
    "    labels: List[int]\n",
    "    vectorizer: Optional[CountVectorizer] = None\n",
    "    is_fitted: bool = False\n",
    "    \n",
    "    def preprocess(self, max_features: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Preprocess the text data by vectorizing it.\n",
    "        \n",
    "        Args:\n",
    "            max_features: Maximum number of features (vocabulary size)\n",
    "        \"\"\"\n",
    "        if not self.vectorizer:\n",
    "            self.vectorizer = CountVectorizer(max_features=max_features)\n",
    "            self.vectorizer.fit(self.texts)\n",
    "            self.is_fitted = True\n",
    "    \n",
    "    def get_features(self) -> np.ndarray:\n",
    "        \"\"\"Convert texts to feature vectors.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Dataset has not been preprocessed. Call preprocess() first.\")\n",
    "        return self.vectorizer.transform(self.texts).toarray()\n",
    "    \n",
    "    def get_tensor_dataset(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get PyTorch tensors for features and labels.\"\"\"\n",
    "        features = self.get_features()\n",
    "        return (\n",
    "            torch.tensor(features, dtype=torch.float32),\n",
    "            torch.tensor(self.labels, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        \"\"\"Get the size of the vocabulary.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Dataset has not been preprocessed. Call preprocess() first.\")\n",
    "        return len(self.vectorizer.get_feature_names_out())\n",
    "    \n",
    "    def transform_new_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Transform new texts using the fitted vectorizer.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Dataset has not been preprocessed. Call preprocess() first.\")\n",
    "        return self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "# Create our dataset and preprocess it\n",
    "dataset = TextDataset(reviews, labels)\n",
    "dataset.preprocess(max_features=100)  # Limit features for the example\n",
    "\n",
    "print(f\"Vocabulary size: {dataset.get_vocabulary_size()}\")\n",
    "features = dataset.get_features()\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Show a sample of the vectorized data\n",
    "sample_idx = 0\n",
    "sample_text = reviews[sample_idx]\n",
    "sample_vector = features[sample_idx]\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Vectorized (first 10 features): {sample_vector[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextDataset` class encapsulates dataset operations in a reusable class. As we cannot work with the reviews in their string format right away, we transform them to numeric vectors using `CountVectorizer`. We convert the raw data into PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properly splitting your data is crucial for reliable model evaluation. This section covers how to create train, validation, and test sets while maintaining consistent preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(\n",
    "    dataset: TextDataset, \n",
    "    val_size: float = 0.15, \n",
    "    test_size: float = 0.15,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[TextDataset, TextDataset, TextDataset]:\n",
    "    \"\"\"\n",
    "    Split a dataset into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The original dataset\n",
    "        val_size: Proportion for validation\n",
    "        test_size: Proportion for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_dataset, val_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    # First split into train+val and test\n",
    "    texts_train_val, texts_test, labels_train_val, labels_test = train_test_split(\n",
    "        dataset.texts, dataset.labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Then split train+val into train and val\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
    "        texts_train_val, labels_train_val, test_size=adjusted_val_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create datasets with the same preprocessing\n",
    "    train_dataset = TextDataset(texts_train, labels_train, dataset.vectorizer, dataset.is_fitted)\n",
    "    val_dataset = TextDataset(texts_val, labels_val, dataset.vectorizer, dataset.is_fitted)\n",
    "    test_dataset = TextDataset(texts_test, labels_test, dataset.vectorizer, dataset.is_fitted)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Split our dataset\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "# Create PyTorch tensors\n",
    "X_train, y_train = train_dataset.get_tensor_dataset()\n",
    "X_val, y_val = val_dataset.get_tensor_dataset()\n",
    "X_test, y_test = test_dataset.get_tensor_dataset()\n",
    "\n",
    "print(f\"Training set: {len(train_dataset.texts)} examples\")\n",
    "print(f\"Validation set: {len(val_dataset.texts)} examples\")\n",
    "print(f\"Test set: {len(test_dataset.texts)} examples\")\n",
    "print(f\"\\nFeature tensor shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed stratified splitting based on the labels of our data. We calculate appropriate split proportions first, and then create dataset objects for each split with shared preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a PyTorch DataLoader\n",
    "Create a function that returns DataLoader objects for training, validation, and test sets. Your implementation should:\n",
    "* Implement proper batching for efficient training\n",
    "* Shuffle training data to improve convergence\n",
    "* Include an option for balanced sampling to handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that returns DataLoader objects for training, validation and test sets\n",
    "# Make sure to implement proper batching and shuffling (for training only)\n",
    "# Include an optional parameter for balanced sampling based on class distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Loss Functions and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing appropriate loss functions and optimizers is critical for effective model training. This section provides a framework for training neural networks in PyTorch.\n",
    "The `train_model` function demonstrates:\n",
    "* Setting up loss functions and optimizers\n",
    "* Implementing the training loop\n",
    "* Tracking training and validation metrics\n",
    "* Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Data class for tracking training results.\"\"\"\n",
    "    train_losses: List[float]\n",
    "    val_losses: List[float]\n",
    "    train_accuracies: List[float]\n",
    "    val_accuracies: List[float]\n",
    "    best_epoch: int\n",
    "    best_val_accuracy: float\n",
    "    training_time: float\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    config: TrainingConfig\n",
    ") -> TrainingResult:\n",
    "    \"\"\"\n",
    "    Train a PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        config: Training configuration\n",
    "        \n",
    "    Returns:\n",
    "        Training results\n",
    "    \"\"\"\n",
    "    device = torch.device(config.device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config.learning_rate, \n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, train_preds = torch.max(outputs, 1)\n",
    "        train_acc = (train_preds == y_train).float().mean().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            \n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_acc = (val_preds == y_val).float().mean().item()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs} | \"\n",
    "              f\"Train Loss: {loss.item():.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    return TrainingResult(\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        train_accuracies=train_accuracies,\n",
    "        val_accuracies=val_accuracies,\n",
    "        best_epoch=best_epoch,\n",
    "        best_val_accuracy=best_val_accuracy,\n",
    "        training_time=training_time\n",
    "    )\n",
    "\n",
    "# Create a text classification model\n",
    "input_size = dataset.get_vocabulary_size()\n",
    "hidden_size = 32\n",
    "output_size = 2  # Binary classification\n",
    "text_model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=4,  # Small batch size for our tiny dataset\n",
    "    epochs=20,\n",
    "    weight_decay=0.0001,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(f\"Training model on {input_size} features...\")\n",
    "training_result = train_model(text_model, X_train, y_train, X_val, y_val, training_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitoring training progress helps identify issues early and tune hyperparameters effectively. This section shows how to visualize training metrics and implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_progress(result: TrainingResult) -> None:\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \n",
    "    Args:\n",
    "        result: Training results\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(result.train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, result.train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, result.val_losses, 'r-', label='Validation Loss')\n",
    "    plt.axvline(x=result.best_epoch + 1, color='g', linestyle='--', label='Best Epoch')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, result.train_accuracies, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, result.val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    plt.axvline(x=result.best_epoch + 1, color='g', linestyle='--', label='Best Epoch')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training progress\n",
    "plot_training_progress(training_result)\n",
    "\n",
    "print(f\"Best validation accuracy: {training_result.best_val_accuracy:.4f} at epoch {training_result.best_epoch+1}\")\n",
    "print(f\"Training time: {training_result.training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function visualizes loss and accuracy over time. With that, we can identify the best model checkpoint, and also detect overfitting and other training issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement learning rate scheduling\n",
    "Learning rate scheduling is an important technique for achieving better convergence, especially in deeper networks.\n",
    "Modify the train_model function to include a learning rate scheduler. Your implementation should:\n",
    "* Implement at least one scheduling strategy (StepLR, ReduceLROnPlateau, or CosineAnnealingLR)\n",
    "* Track and plot learning rate changes over epochs\n",
    "* Compare training with and without scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the train_model function to include a learning rate scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Reproducibility and Experiment Tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintaining reproducibility and tracking experiments is crucial for research and development. This section introduces a framework for experiment configuration and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Data class for experiment configuration.\"\"\"\n",
    "    experiment_name: str\n",
    "    model_type: str\n",
    "    model_params: Dict\n",
    "    training_params: TrainingConfig\n",
    "    dataset_params: Dict\n",
    "    seed: int = 42\n",
    "    \n",
    "    def save(self, filepath: str) -> None:\n",
    "        \"\"\"Save experiment configuration to a JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            # Convert all dataclasses to dictionaries\n",
    "            config_dict = {\n",
    "                'experiment_name': self.experiment_name,\n",
    "                'model_type': self.model_type,\n",
    "                'model_params': self.model_params,\n",
    "                'training_params': self.training_params.to_dict(),\n",
    "                'dataset_params': self.dataset_params,\n",
    "                'seed': self.seed\n",
    "            }\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> 'ExperimentConfig':\n",
    "        \"\"\"Load experiment configuration from a JSON file.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "            # Convert dictionaries back to dataclasses\n",
    "            training_params = TrainingConfig(**config_dict['training_params'])\n",
    "            return cls(\n",
    "                experiment_name=config_dict['experiment_name'],\n",
    "                model_type=config_dict['model_type'],\n",
    "                model_params=config_dict['model_params'],\n",
    "                training_params=training_params,\n",
    "                dataset_params=config_dict['dataset_params'],\n",
    "                seed=config_dict['seed']\n",
    "            )\n",
    "\n",
    "# Example: Creating an experiment configuration\n",
    "experiment_config = ExperimentConfig(\n",
    "    experiment_name=\"text_classification_bow\",\n",
    "    model_type=\"SimpleNN\",\n",
    "    model_params={\n",
    "        \"input_size\": input_size,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"output_size\": output_size\n",
    "    },\n",
    "    training_params=training_config,\n",
    "    dataset_params={\n",
    "        \"max_features\": 100,\n",
    "        \"train_size\": len(train_dataset.texts),\n",
    "        \"val_size\": len(val_dataset.texts),\n",
    "        \"test_size\": len(test_dataset.texts)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save experiment configuration\n",
    "os.makedirs(\"experiments\", exist_ok=True)\n",
    "config_path = os.path.join(\"experiments\", \"experiment_config.json\")\n",
    "experiment_config.save(config_path)\n",
    "print(f\"Saved experiment configuration to {config_path}\")\n",
    "\n",
    "def log_metrics(result: TrainingResult, log_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Log training metrics to a file.\n",
    "    \n",
    "    Args:\n",
    "        result: Training results\n",
    "        log_path: Path to log file\n",
    "    \"\"\"\n",
    "    with open(log_path, 'w') as f:\n",
    "        metrics = {\n",
    "            \"train_losses\": result.train_losses,\n",
    "            \"val_losses\": result.val_losses,\n",
    "            \"train_accuracies\": result.train_accuracies,\n",
    "            \"val_accuracies\": result.val_accuracies,\n",
    "            \"best_epoch\": result.best_epoch,\n",
    "            \"best_val_accuracy\": result.best_val_accuracy,\n",
    "            \"training_time\": result.training_time,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Log metrics from our experiment\n",
    "log_path = os.path.join(\"experiments\", \"metrics.json\")\n",
    "log_metrics(training_result, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our basic implementation provides fundamental tracking capabilities, for larger projects and team environments, external experiment tracking tools offer significant advantages, e.g., Weights & Biases (wandb), MLFlow, or TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Model Saving and Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to save model weights and architecture, load models for inference and include metadata about the training conditions.\n",
    "As your models become more complex and training runs take longer, proper experiment tracking becomes essential for reproducibility, comparison, collaboration, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model: nn.Module, filepath: str, metadata: Dict = None) -> None:\n",
    "    \"\"\"\n",
    "    Save model weights and optional metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        filepath: Path to save the model\n",
    "        metadata: Optional metadata dictionary\n",
    "    \"\"\"\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'metadata': metadata or {}\n",
    "    }\n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model_class: nn.Module, filepath: str, **model_params) -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Load model weights and metadata.\n",
    "    \n",
    "    Args:\n",
    "        model_class: PyTorch model class\n",
    "        filepath: Path to the saved model\n",
    "        **model_params: Parameters to initialize the model\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, metadata)\n",
    "    \"\"\"\n",
    "    # Create a new model instance\n",
    "    model = model_class(**model_params)\n",
    "    \n",
    "    # Load the saved state\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, checkpoint['metadata']\n",
    "\n",
    "# Save our trained model\n",
    "model_metadata = {\n",
    "    \"accuracy\": training_result.best_val_accuracy,\n",
    "    \"epoch\": training_result.best_epoch,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"vocabulary_size\": input_size\n",
    "}\n",
    "\n",
    "model_path = os.path.join(\"experiments\", \"text_classifier.pt\")\n",
    "save_model(text_model, model_path, model_metadata)\n",
    "\n",
    "# Load the model for inference\n",
    "loaded_model, metadata = load_model(\n",
    "    SimpleNN, \n",
    "    model_path, \n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print(f\"Loaded model trained to accuracy: {metadata['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll demonstrate how to use a trained model for inference on new data. This section covers the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentiment(\n",
    "    model: nn.Module, \n",
    "    vectorizer: CountVectorizer, \n",
    "    text: str,\n",
    "    device: str = \"cpu\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Predict sentiment of a text.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        vectorizer: Fitted CountVectorizer\n",
    "        text: Input text\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Vectorize the input text\n",
    "    features = vectorizer.transform([text]).toarray()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    # Get class probabilities\n",
    "    class_probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"positive_probability\": float(class_probs[1]),\n",
    "        \"negative_probability\": float(class_probs[0])\n",
    "    }\n",
    "\n",
    "# Test the model with some new reviews\n",
    "test_reviews = [\n",
    "    \"The movie was fantastic and I would watch it again. Highly recommended!\",\n",
    "    \"A complete waste of time with terrible acting, I hated it and would not recommend it\",\n",
    "    \"Not great, not terrible, just an average film\"\n",
    "]\n",
    "\n",
    "print(\"Running inference on new reviews:\")\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(loaded_model, dataset.vectorizer, review)\n",
    "    sentiment = \"Positive\" if result[\"predicted_class\"] == 1 else \"Negative\"\n",
    "    print(f\"\\nText: '{result['text']}'\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {max(result['positive_probability'], result['negative_probability']):.4f})\")\n",
    "    print(f\"Probabilities: Positive = {result['positive_probability']:.4f}, Negative = {result['negative_probability']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code demonstrates:\n",
    "* Preprocessing new data consistently\n",
    "* Running inference efficiently\n",
    "* Interpreting model outputs\n",
    "* Evaluating model performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources and Best Practices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've covered the fundamentals of neural networks with PyTorch, from basic architecture to training, evaluation, and deployment. These concepts form the foundation for working with more complex models like LLMs in future exercises.\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "* [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Official documentation for PyTorch\n",
    "* [Deep Learning with PyTorch](https://isip.piconepress.com/courses/temple/ece_4822/resources/books/Deep-Learning-with-PyTorch.pdf) - Comprehensive book on PyTorch\n",
    "* [Dive into Deep Learning](https://d2l.ai/) - Interactive deep learning book with code examples\n",
    "* [Weights & Biases](https://wandb.ai/site) - Tool for experiment tracking and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Assessment\n",
    "Complete the following self-assessment to gauge your understanding:\n",
    "\n",
    "1. Explain the difference between model.train() and model.eval() modes\n",
    "2. Why is torch.no_grad() important during inference?\n",
    "3. What are the benefits of using data classes in ML projects?\n",
    "4. What steps are necessary to ensure reproducibility in ML experiments?\n",
    "5. Why is proper train/validation/test splitting important?\n",
    "6. How would you modify our text classifier to handle longer texts (hint: think about the limitations of Bag-of-Words)\n",
    "\n",
    "Bonus: Try implementing a simple Transformer model using PyTorch's nn.TransformerEncoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
