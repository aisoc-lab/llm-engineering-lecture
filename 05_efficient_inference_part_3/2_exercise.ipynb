{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# Cache sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate bitsandbytes --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75af266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute this if you are using the machine at jupyterhub.uni-muenster.de\n",
    "# These machine only have 10GB disk storage which gets filled quite quickly.\n",
    "\n",
    "# ! rm -r ~/.cache/pip\n",
    "# ! rm -r ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a71a9d",
   "metadata": {},
   "source": [
    "# Exercise 1: Comparing the Dynamic and Sliding Window Cache [30 mins]\n",
    "\n",
    "In this exercise, we will load a 8 bit quantized model using `bitsandbytes`. Make sure you have access to a GPU since `bitsandbytes` does not yet support CPUs.\n",
    "\n",
    "First load, the model in 8 bits using the code below. Next, generate some long outputs using prompts like `Write a really long story`. Sample `10` sequences with a temperature of `0.5`. How long does the generation take? Are the generations coherent?\n",
    "\n",
    "Recall that HuggingFace uses the dynamic cache by default where memory is allocated for cache on the fly. Now we will simulate a scenario where we are memory constrained. Use a [`SlidingWindowCache`](https://huggingface.co/docs/transformers/v4.51.3/en/internal/generation_utils#transformers.SlidingWindowCache). What kind of differences do you notice in runtime and coherence? Does changing the parameters help?\n",
    "\n",
    "Tip: Feel free to use datasets from HuggingFace like [discrim-eval](https://huggingface.co/datasets/Anthropic/discrim-eval) that consist of longish prompts. Add instructions like \"think step by step\" to encourage the model to generate long outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4b4db",
   "metadata": {},
   "source": [
    "# Exercise 2: Using the Sink Cache [30 mins]\n",
    "\n",
    "Now generate using the [`SinkCache`](https://huggingface.co/docs/transformers/v4.51.3/en/internal/generation_utils#transformers.SinkCache). Compare the runtime and generation quality with the dynamic and sliding window caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc56b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
