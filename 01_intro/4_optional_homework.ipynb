{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code snippet from exercise 1\n",
    "# Set the seed for reproducibility\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1: Recommended Reading: Understanding Attention\n",
    "\n",
    "To build strong foundations in how modern Language Models work and to understand the **attention mechanism**, read:\n",
    "\n",
    "[The Transformer Family - Lilian Weng](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)\n",
    "\n",
    "Answer the following questions:\n",
    "* What is the role of attention in a Transformer?\n",
    "* How does self-attention work?\n",
    "* How does attention differ from previous mechanisms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: Sampling Strategies for Language Generation\n",
    "\n",
    "When generating text with a language model, we don't just want the most likely next word every timeâ€”that would make outputs boring and repetitive. Instead, we use sampling to inject creativity and variation.\n",
    "\n",
    "Two of the most common sampling strategies are:\n",
    "\n",
    "**Top-k Sampling**\n",
    "Instead of choosing from all possible tokens, we limit the model to the k most likely next tokens. This narrows the focus to a small, high-confidence pool while still allowing for variation.\n",
    "\n",
    "**Top-p (Nucleus) Sampling**\n",
    "Rather than picking a fixed number k, top-p sampling looks at the smallest set of tokens whose cumulative probability exceeds p (e.g., 90%). It adapts to the uncertainty of the model: if the model is confident, fewer tokens are considered; if it's unsure, more are included.\n",
    "\n",
    "These techniques help balance randomness and relevance in generation.\n",
    "\n",
    "### (2a)\n",
    "ðŸ“˜ **Reading Assignment:**\n",
    "To understand the different decoding methods for language generation, read this short article:\n",
    "ðŸ‘‰ [Huggingface Blog](https://huggingface.co/blog/how-to-generate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b)\n",
    "In this task, youâ€™ll gain hands-on experience with text generation using pre-trained LLMs and explore how different sampling strategies affect the model's output.\n",
    "You will implement a custom text generation function using Hugging Faceâ€™s transformers library, and evaluate the impact of temperature, top-k, and top-p sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model and Tokenizer Setup\n",
    "Load a pre-trained **causal language model** and its **tokenizer** using Hugging Faceâ€™s `AutoModelForCausalLM` and `AutoTokenizer`.\n",
    "We can reuse the code from Exercise 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement a custom text generation function\n",
    "Your function should:\n",
    "* Accept a text prompt and parameters:\n",
    "    - `gen_len`: number of tokens to generate\n",
    "    - `temperature`: value to scale logits (controls randomness)\n",
    "    - `top_k`: limits sampling to the top-k most probable tokens\n",
    "    - `top_p`: applies nucleus sampling, selecting tokens whose cumulative probability exceeds `p`\n",
    "\n",
    "* Apply:\n",
    "    - Temperature scaling  \n",
    "    - top-k filtering  \n",
    "    - top-p (nucleus) filtering (with cumulative probability logic)\n",
    "\n",
    "Use PyTorch or Hugging Face utilities where possible, but ensure you understand and explain the logic behind the sampling.\n",
    "\n",
    "*Hint:* You can reuse some of the code from exercise 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Bochum because it is a beautiful city with a lot of history. The city is located in the middle of the Rhine and is surrounded by beautiful forests. The'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a code snippet from exercise 1\n",
    "def generate(prompt: str, gen_len: int) -> str:\n",
    "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_input[\"input_ids\"]\n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "           output = model(input_ids).logits  # We should pass the attention mask but we can ignore it for causal LLMs when we have just a single input\n",
    "        output = output.squeeze(dim=0)\n",
    "        next_token_scores = output[-1]\n",
    "        next_token_id = next_token_scores.argmax(dim=-1)\n",
    "        input_ids = torch.cat((input_ids, torch.LongTensor([next_token_id]).reshape(1,-1)), dim=-1)\n",
    "    return tokenizer.decode(input_ids.numpy().flatten())\n",
    "\n",
    "prompt = \"I love Bochum because\"\n",
    "generate(prompt, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Experiment\n",
    "For a fixed prompt, generate text under the following conditions:\n",
    "- Temperature values: `0.0`, `0.5`, `1.0`, `1.5`, \n",
    "- Top-k values: `5`, `10`, `15`\n",
    "- Top-p values: `0.8`, `0.9`, `0.95`\n",
    "\n",
    "Print the outputs and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflect & Analyze\n",
    "Observe how the generated texts differ under various settings.  \n",
    "For each variation, discuss:\n",
    "* How did the output change in diversity, coherence, and creativity?\n",
    "* Which combination(s) produced the most interesting or usable results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
