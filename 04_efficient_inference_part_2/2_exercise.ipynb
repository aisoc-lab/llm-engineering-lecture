{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# A deeper look into quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate bitsandbytes --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75af266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute this if you are using the machine at jupyterhub.uni-muenster.de\n",
    "# These machine only have 10GB disk storage which gets filled quite quickly.\n",
    "\n",
    "# ! rm -r ~/.cache/pip\n",
    "# ! rm -r ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a71a9d",
   "metadata": {},
   "source": [
    "# Exercise 1: Loading a LLM.int8() quantized model [20 mins]\n",
    "\n",
    "In this exercise, we will load a 8 bit quantized model using `bitsandbytes`. Make sure you have access to a GPU since `bitsandbytes` does not yet support CPUs.\n",
    "\n",
    "1. First load the base model using the code below.\n",
    "\n",
    "2. Next, load the quantized model using [bitsandbytes](https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes).\n",
    "\n",
    "Compare the size of the two models using the `model.get_memory_footprint()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"stabilityai/stablelm-zephyr-3b\" # Use if you have access to a GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to load the quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4b4db",
   "metadata": {},
   "source": [
    "# Exercise 2: Comparing the quantized and the original model [30 mins]\n",
    "\n",
    "Download the [dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset from huggingface. Select 100 random samples and compare the (i) runtime and (ii) outputs of the original and quantized model.\n",
    "\n",
    "What happens when you use a 4-bit quantization instead of 8-bits?\n",
    "\n",
    "**Note**: If you GPU RAM is very limited, you might be able to load only one model at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc56b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff4844",
   "metadata": {},
   "source": [
    "# Exercise 3: Calculating perplexity [20 mins]\n",
    "\n",
    "Download the [BOLD dataset](https://huggingface.co/datasets/AmazonScience/bold/). Select the `wikipedia` column from 100 random samples. Compute the [perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) of the model on these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8f5f3",
   "metadata": {},
   "source": [
    "# Exercise 4: Tinkering with quantization parameters [25 mins]\n",
    "\n",
    "Try different quantization levels (8 vs. 4 bits). Also, try different values of `outlier_threshold` in the [bitsanbytes config](https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes).SystemError\n",
    "\n",
    "What differences do you observe in the generations and the model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d963be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
