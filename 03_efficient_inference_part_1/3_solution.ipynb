{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# Efficient inference with Tranformers\n",
    "\n",
    "In this exercise, we will be looking more closely into our LLM. We will explore strategies for reducing the size of our models and its impact on the quality of model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4268a",
   "metadata": {},
   "source": [
    "# Exercise 1: Estimating the size of our model [30 mins]\n",
    "\n",
    "Recall your a HuggingFace `model` is a simple PyTorch NN. Inspect the model by doing the following:\n",
    "\n",
    "1. Print the total number of layers.\n",
    "2. Names of all the parameter tensors inside each layer, along with their data type.\n",
    "3. The output dimensionality at each layer along with it data type.\n",
    "4. The data type of each tensor and the estimated storage requirements in MBs.\n",
    "5. The total size of the model in MBs.\n",
    "\n",
    "**Hint:** You can use `print(model)` to see all the layers and sublayers inside the model.\n",
    "\n",
    "**Hint:** Recall that all the layers and the model itself are simply `torch.nn` modules. Which means you can call the functions `model.named_parameters()` or `layer.named_parameters()` to go over the parameters or the model or the layer. See the [documentation of `named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\" # Very small model that we used in the last class. Can be used without a GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(model) # Printing the model will give you an overview of its layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc84626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "parameter_names = []\n",
    "parameter_sizes = []\n",
    "num_elements = []\n",
    "element_sizes = []\n",
    "\n",
    "# The lm_head is not counted as a named parameter of the model. Not sure why.\n",
    "# So we iterate over both the named_parameters of model and model.lm_head.\n",
    "for pname, pval in itertools.chain(model.named_parameters(), model.lm_head.named_parameters()):\n",
    "    num_params = pval.numel()  # number of elements in this parameter tensor\n",
    "    param_size = pval.element_size()  # Size of a single element in bytes\n",
    "    parameter_names.append(pname)\n",
    "    parameter_sizes.append(num_params * param_size)  # Size of this tensor\n",
    "    num_elements.append(num_params)\n",
    "    element_sizes.append(param_size)\n",
    "\n",
    "print(f\"Total number of parameters: {sum(num_elements)}\")\n",
    "print(f\"Total model size in Bytes: {sum(parameter_sizes)}\")\n",
    "print(f\"Size in bytes of parameters tensors with counts: {Counter(element_sizes)} \")\n",
    "\n",
    "element_count_sorted = sorted(zip(parameter_names, num_elements), key=lambda x: x[1], reverse=True)\n",
    "print(f\"Tensor with largest num_params: {element_count_sorted[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7cfe5",
   "metadata": {},
   "source": [
    "# Exercise 2: Converting the model to bfloat [10 mins]\n",
    "\n",
    "You might have noticed that the `AutoModelForCausalLM.from_pretrained` loaded the model in `torch.float32` data type. Load the model again, this time by passing the augument `torch_dtype=torch.bfloat16` to the `from_pretrained` method.\n",
    "\n",
    "Compute the size of individual layers and the total model size again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98068f0d",
   "metadata": {},
   "source": [
    "**Solution:** Just pass the `torch_dtype=bfloat16` to the `from_pretrained` method and run the previous code cell again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0892e",
   "metadata": {},
   "source": [
    "# Exercise 3: Capturing different aspects of model outputs [20 mins]\n",
    "\n",
    "We would like to compare the difference in the parameters and outputs of the models when we quantize them.\n",
    "\n",
    "Write a function that given a prompt, returns the following stats for the original model that is loaded in `torch.float32` precision.\n",
    "\n",
    "1. First N output tokens generated in a greedy manner.\n",
    "2. Softmax probabilities at each generation position. In other words, compute these probabilities not just for output tokens but _also for input tokens_.\n",
    "3. Weights at the Kth transformer layer where K is a parameter to the function.\n",
    "\n",
    "Make sure your function always returns these all tensors in `float32` precision.\n",
    "\n",
    "**Hint:** You can use the generate function that you write in one of the previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "        model: torch.nn.Module,\n",
    "        prompt: str,\n",
    "        gen_len: int,\n",
    "        layer_number: int,\n",
    "        pname: str = \"mlp.up_proj.weight\",\n",
    "    ) -> [str, torch.FloatTensor, torch.FloatTensor]:\n",
    "    # Your code here\n",
    "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_input[\"input_ids\"]\n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "           output = model(input_ids).logits  # We should pass the attention mask but we can ignore it for causal LLMs when we have just a single input\n",
    "        output = output.squeeze(dim=0)\n",
    "        next_token_id = torch.argmax(output[-1], dim=-1)  # Most likely token at the final input token\n",
    "        input_ids = torch.cat((input_ids, torch.LongTensor([next_token_id]).reshape(1,-1)), dim=-1)\n",
    "    gen = tokenizer.decode(input_ids.numpy().flatten()) # Includes the input too\n",
    "    softmax_probs = torch.softmax(output, dim=-1).detach().to(torch.float32)\n",
    "    w = model.get_parameter(f\"model.layers.{layer_number}.{pname}\")\n",
    "    return gen, softmax_probs, w.to(torch.float32)\n",
    "\n",
    "prompt = \"Einstein was born in\"\n",
    "gen, softmax_probs, w = generate(model, prompt, 10, 10, \"self_attn.q_proj.weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d707c6",
   "metadata": {},
   "source": [
    "# Exercise 4: Comparing differences in model outputs [20 mins]\n",
    "\n",
    "Use three different versions of the model:\n",
    "1. The originally loaded model in `torch.float32` format.\n",
    "2. A model that you manually cast to `torch.float16` using [`model.half()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half) function.\n",
    "3. A model that you load in `torch.bfloat16` by passing the `torch.dtype=torch.bfloat16` parameter to the pretrained model.\n",
    "\n",
    "Compare the following properties of these three models:\n",
    "1. Difference in generated tokens.\n",
    "2. 10 most likely tokens at each generation position.\n",
    "3. L2 norm of difference in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f285715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "prompt = \"Einstein was born in\"\n",
    "gen_len = 20\n",
    "layer_num = 20\n",
    "#pname = \"self_attn.q_proj.weight\"\n",
    "pname = \"mlp.up_proj.weight\"\n",
    "\n",
    "model_fp16 = model.half()\n",
    "model_bf16 = model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "params = (prompt, gen_len, layer_num)\n",
    "k_params = {\"pname\": pname}\n",
    "g_base, s_base, w_base = generate(model, *params, **k_params)\n",
    "g_fp16, s_fp16, w_fp16 = generate(model_fp16, *params, **k_params)\n",
    "g_bf16, s_bf16, w_bf16 = generate(model_bf16, *params, **k_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b573166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP 32 vs. FP16\n",
    "labels = [\"FP32\", \"FP16\", \"BF16\"]\n",
    "gens = [g_base, g_fp16, g_bf16]\n",
    "softmaxes = [s_base, s_fp16, s_bf16]\n",
    "weights = [w_base, w_fp16, w_bf16]\n",
    "\n",
    "print(\"= Generations =\")\n",
    "for mname, g in zip(labels, gens):\n",
    "    print(f\"{mname} -- {g}\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"= Softmax =\")\n",
    "for mname, s in zip(labels, softmaxes):\n",
    "    print(f\"- {mname} -\")\n",
    "    print(s.max(-1).values.to(torch.float32).numpy())\n",
    "print(\"\\n\\n\")\n",
    "print(\"= Weight diff from base =\")\n",
    "for mname, w in zip(labels, weights):\n",
    "    print(f\"- {mname} -\")\n",
    "    print(f\"{((w - w_base).abs()).sum().detach().numpy()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
