{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding with Generative Models: What to Remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with generative language models in HuggingFace Transformers, padding is crucial for batching and consistent input lengths. However, improper handling can lead to unexpected outputs or degraded performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR\n",
    "* set the padding side to left\n",
    "* set the correct padding token\n",
    "* set the attention mask to tell the model which tokens are padding and should be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO adjust to your GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set padding_side to \"left\".\n",
    "\n",
    "Explanation: Causal models generate tokens autoregressively from left to right. Left padding ensures that the prompt is always at the rightmost position, so the model can attend to it properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the pad token = eos token.\n",
    "\n",
    "Explanation: For Llama and similar models, the EOS token is often used as the pad token. For other models, check the tokenizer config or documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elisabeth/anaconda3/envs/infaccnew/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe81a77476fd49e1b7f97825b375de15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# We will use the same tokenizer, model, and prompt across all conditions.\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "MODEL.to(\"cuda\")\n",
    "PROMPT = \"Hello, how are you?\"\n",
    "MAX_NEW_TOKENS = 100\n",
    "\n",
    "TOKENIZER.pad_token = TOKENIZER.eos_token\n",
    "TOKENIZER.padding_side = \"left\"\n",
    "\n",
    "MODEL.config.pad_token_id = TOKENIZER.eos_token_id\n",
    "\n",
    "\n",
    "def generate_text(input_ids, attention_mask):\n",
    "    output = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=MAX_NEW_TOKENS, pad_token_id=TOKENIZER.eos_token_id, do_sample=False, temperature=None, top_k=None, top_p=None)\n",
    "    return TOKENIZER.decode(output[0][len(input_ids[0]):], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate text without padding.\n",
    "When we pad the input, we want to get the exact same output from the model as without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input: <|begin_of_text|>Hello, how are you?\n",
      "Output without padding:  I hope you are well. I am writing to you because I am looking for a job. I am a hard worker and I am very responsible. I am a good person and I am very friendly. I am looking for a job because I need to support my family. I am a good cook and I am very good at cleaning. I am also very good at taking care of children. I am looking for a job that will allow me to use my skills and help me to support my family\n"
     ]
    }
   ],
   "source": [
    "# NO PADDING\n",
    "input = TOKENIZER(PROMPT, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Decoded input: {TOKENIZER.decode(input['input_ids'][0])}\")\n",
    "\n",
    "output_no_padding = generate_text(input_ids=input[\"input_ids\"], attention_mask=None)\n",
    "\n",
    "print(f\"Output without padding: {output_no_padding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply padding to max_length. We set the attention mask to ignore these padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input: <|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>Hello, how are you?\n",
      "Output with padding:  I hope you are well. I am writing to you because I am looking for a job. I am a hard worker and I am very responsible. I am a good person and I am very friendly. I am looking for a job because I need to support my family. I am a good cook and I am very good at cleaning. I am also very good at taking care of children. I am looking for a job that will allow me to use my skills and help me to support my family\n"
     ]
    }
   ],
   "source": [
    "# PADDING AND ATTENTION MASK\n",
    "input = TOKENIZER(PROMPT, return_tensors=\"pt\", padding=\"max_length\", max_length=100).to(\"cuda\")\n",
    "\n",
    "print(f\"Decoded input: {TOKENIZER.decode(input['input_ids'][0])}\")\n",
    "\n",
    "# this is what the attention mask should look like (all 1s except for the padding tokens, which are 0s)\n",
    "test_attention_mask = torch.where(input['input_ids'] == TOKENIZER.pad_token_id, torch.zeros_like(input['input_ids']), torch.ones_like(input['input_ids']))\n",
    "assert torch.all(input[\"attention_mask\"] == test_attention_mask)\n",
    "\n",
    "output_padding = generate_text(input_ids=input[\"input_ids\"], attention_mask=input[\"attention_mask\"])\n",
    "print(f\"Output with padding: {output_padding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is generating the same output as without padding (as it should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output_no_padding == output_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What not to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Not setting the pad token:** If you donâ€™t set these, you may get warnings or errors, and the model may treat padding tokens as valid input.\n",
    "2. **Not passing attention mask:** If you omit the attention_mask, the model may attend to padding tokens, leading to poor or unpredictable generations (see 1st example below).\n",
    "3. **Using right padding with causal models:** Causal models expect left padding. Right padding can cause the model to ignore the prompt or generate poor outputs (see 2nd example below).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input: <|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>Hello, how are you?\n",
      "Output:  I'm? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I? I\n"
     ]
    }
   ],
   "source": [
    "# BAD: Padding, but no attention mask\n",
    "input = TOKENIZER(PROMPT, return_tensors=\"pt\", padding=\"max_length\", max_length=100).to(\"cuda\")\n",
    "\n",
    "print(f\"Decoded input: {TOKENIZER.decode(input['input_ids'][0])}\")\n",
    "\n",
    "output = generate_text(input_ids=input[\"input_ids\"], attention_mask=None)\n",
    "\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input: <|begin_of_text|>Hello, how are you?<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n",
      "Output: <|begin_of_text|>://\n",
      "I am a 3D artist and I am looking for a job. I have a lot of experience in 3D modeling, texturing, rigging, animation, and rendering. I have worked on a lot of projects and I am very good at what I do. I am looking for a job that will allow me to use my skills and experience to create amazing 3D models and animations. I am a hard worker and I am always looking for ways to improve my\n"
     ]
    }
   ],
   "source": [
    "# BAD: Set padding side to \"right\", yields a different output\n",
    "TOKENIZER.padding_side = \"right\"\n",
    "\n",
    "input = TOKENIZER(PROMPT, return_tensors=\"pt\", padding=\"max_length\", max_length=100).to(\"cuda\")\n",
    "\n",
    "print(f\"Decoded input: {TOKENIZER.decode(input['input_ids'][0])}\")\n",
    "\n",
    "output = generate_text(input_ids=input[\"input_ids\"], attention_mask=input[\"attention_mask\"])\n",
    "\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "| Scenario                | Output Quality | Notes                        |\n",
    "|-------------------------|---------------|------------------------------|\n",
    "| No padding              | Good          | Baseline                     |\n",
    "| Left padding + mask     | Good          | Matches baseline             |\n",
    "| Left padding, no mask   | Bad           | Model attends to padding     |\n",
    "| Right padding + mask    | Bad           | Does not match      \n",
    "         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM and Padding\n",
    "Lastly, let's look at how vLLM handles padding automatically and efficiently. Unlike the manual approach we saw earlier, vLLM abstracts away the complexity of padding management, making it easy to work with batched inference while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the GPU cache\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "del MODEL\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 07:20:19 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-03 07:20:28 [config.py:823] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-03 07:20:29 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-03 07:20:30 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 07-03 07:20:31 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elisabeth/anaconda3/envs/infaccnew/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 07:20:34 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-03 07:20:35 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-03 07:20:35 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='meta-llama/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-03 07:20:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x744b7800f9a0>\n",
      "INFO 07-03 07:20:36 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-03 07:20:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-03 07:20:36 [gpu_model_runner.py:1595] Starting to load model meta-llama/Meta-Llama-3.1-8B...\n",
      "INFO 07-03 07:20:37 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-03 07:20:37 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-03 07:20:37 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.22it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 07:20:40 [default_loader.py:272] Loading weights took 2.73 seconds\n",
      "INFO 07-03 07:20:41 [gpu_model_runner.py:1624] Model loading took 14.9889 GiB and 3.948407 seconds\n",
      "INFO 07-03 07:20:46 [backends.py:462] Using cache directory: /home/elisabeth/.cache/vllm/torch_compile_cache/260813549e/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-03 07:20:46 [backends.py:472] Dynamo bytecode transform time: 4.88 s\n",
      "INFO 07-03 07:20:50 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 3.660 s\n",
      "INFO 07-03 07:20:51 [monitor.py:34] torch.compile takes 4.88 s in total\n",
      "INFO 07-03 07:20:52 [gpu_worker.py:227] Available KV cache memory: 105.91 GiB\n",
      "INFO 07-03 07:20:52 [kv_cache_utils.py:715] GPU KV cache size: 867,600 tokens\n",
      "INFO 07-03 07:20:52 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 6.62x\n",
      "INFO 07-03 07:21:10 [gpu_model_runner.py:2048] Graph capturing finished in 18 secs, took 0.66 GiB\n",
      "INFO 07-03 07:21:10 [core.py:171] init engine (profile, create kv cache, warmup model) took 28.84 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Using vLLM with proper padding\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B\", device=\"cuda\")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e085a5ebe34e8aa9ec1dca4747fd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041314fb2a4e40748c88a26177161706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  I hope you are well. I am writing to you because I am looking for a job. I am a hard worker and I am very responsible. I am a good person and I am very friendly. I am looking for a job because I need to support my family. I am a good cook and I am very good at cleaning. I am also very good at taking care of children. I am looking for a job that will allow me to use my skills and help me to support my family\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare prompts\n",
    "prompts = [\n",
    "    PROMPT,\n",
    "    \"A longer prompt that will require padding\"\n",
    "]\n",
    "\n",
    "# vLLM handles padding automatically and efficiently\n",
    "# No need to manually set pad tokens or attention masks\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "output_vllm = outputs[0].outputs[0].text\n",
    "\n",
    "# Print results\n",
    "print(f\"Output: {output_vllm}\")\n",
    "\n",
    "\n",
    "# vLLM will automatically handle padding and attention masks\n",
    "# during batch processing for optimal performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the output from vllm is the same as without padding and as with the manual approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output_no_padding == output_vllm == output_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "* [HuggingFace Docs: Padding & Attention Mask](https://huggingface.co/docs/transformers/pad_truncation)\n",
    "* [HuggingFace Forums: Padding for CausalLM](https://discuss.huggingface.co/t/the-effect-of-padding-side/67188)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infaccnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
