{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LLM Training Workloads\n",
    "\n",
    "In the lecture we looked at how to (theoretically) calculate FLOPs and built a mental model around where much of compute is spent. We also tried to build intuition about how compute patterns massively differ during LLM training and inference. Here we will try and actually analyze some simple training and inference workloads on a Llama-like model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch>=2.0.0 transformers>=4.30.0 datasets>=2.10.0 accelerate>=0.20.0  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# HF Trainer complains and tries to initialze distributed, we set these flags to make sure we train on one GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "\n",
    "# Disable wandb and other logging services completely\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Huggingface inference code for a pre-trained model\n",
    "\n",
    "You've probably seen this many times by now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id: str) -> AutoModelForCausalLM:\n",
    "    return AutoModelForCausalLM.from_pretrained(model_id).cuda()\n",
    "\n",
    "def load_tokenizer(model_id) -> AutoTokenizer:\n",
    "    return AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def inference(model, tokenizer, inputs: List[str]):\n",
    "    model_inputs = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs, \n",
    "                max_new_tokens=30,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Germany?\n",
      "Berlin is the capital of Germany.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = load_model(model_id)\n",
    "tokenizer = load_tokenizer(model_id)\n",
    "print (inference(model, tokenizer, [\"What is the capital of Germany\"]))\n",
    "\n",
    "# free up the GPU\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, max_length=512):\n",
    "    \"\"\"Load and prepare a simple dataset from HF Hub\"\"\"\n",
    "    \n",
    "    # Using a small, simple dataset\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    \n",
    "    # Take only a small subset for demo (first 1000 examples)\n",
    "    dataset = dataset.select(range(1000))\n",
    "    print(f\"Dataset size: {len(dataset)} examples\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Tokenize the text data\"\"\"\n",
    "        # Tokenize and truncate to max_length\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"], \n",
    "            truncation=True, \n",
    "            padding=False, \n",
    "            max_length=max_length,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    # Split into train/eval (90/10 split)\n",
    "    train_size = int(0.9 * len(tokenized_dataset))\n",
    "    train_dataset = tokenized_dataset.select(range(train_size))\n",
    "    eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "    \n",
    "    print(f\"Train examples: {len(train_dataset)}\")\n",
    "    print(f\"Eval examples: {len(eval_dataset)}\")\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def train(config: dict):\n",
    "    print(\"üöÄ Starting Configurable Llama Training (Single GPU)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    BATCH_SIZE = 1       # Adjust based on GPU memory\n",
    "    MAX_LENGTH = 256     # Sequence length\n",
    "    \n",
    "    config = LlamaConfig(**config)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Load tokenizer (we'll use Llama tokenizer but create our own model)\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    \n",
    "    # 2. Create model with random initialization\n",
    "    print(\"Creating model with random initialization...\")\n",
    "    model = LlamaForCausalLM(config).to(dtype=torch.bfloat16, device=device)\n",
    "    \n",
    "    # Add padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"üìä Actual model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"üéØ Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # 4. Prepare dataset\n",
    "    train_dataset, eval_dataset = prepare_dataset(tokenizer, max_length=MAX_LENGTH)\n",
    "    \n",
    "    # 5. Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    \n",
    "    # 6. Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./toy-llama-training\",\n",
    "        overwrite_output_dir=True,\n",
    "        \n",
    "        # Training setup\n",
    "        max_steps=10,  # Only 10 steps for profiling\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=5e-4,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Logging and evaluation\n",
    "        logging_steps=5,     \n",
    "        eval_steps=50,       # Disable eval during short profiling run\n",
    "        save_steps=50,       # Disable saving during short profiling run\n",
    "        eval_strategy=\"no\",  # Disable evaluation for profiling\n",
    "        save_strategy=\"no\",  # Disable saving for profiling\n",
    "        \n",
    "        # Single GPU settings\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0,\n",
    "        local_rank=-1,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        \n",
    "        # Misc\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=None,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîß Training Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"   ‚Ä¢ Max Sequence Length: {MAX_LENGTH}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {training_args.learning_rate}\")\n",
    "    print(f\"   ‚Ä¢ Mixed Precision: {training_args.bf16}\")\n",
    "    \n",
    "    # 7. Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # 8. Train the model\n",
    "    print(\"\\nüèãÔ∏è Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Fit model on the GPU by playing with config parameters [20 mins]\n",
    "\n",
    "### 1A. Plot peak memory as a function of num_hidden_layers (number of transformer layers)\n",
    "\n",
    "### 1B. Plot peak memory as a function of hidden_size (size of model embedding)\n",
    "\n",
    "### 1C. Plot peak memory as a function of intermediate_size (This is the hidden size of MLP)\n",
    "\n",
    "On Colab, the GPU is not large enough to fit a 1B llama model for training, play with parameters in the config to make the model, grads, optimizer, activations train on one GPU. \n",
    "\n",
    "Hint: recall where most of the parameter count lies\n",
    "\n",
    "Hint: For peak memory you can use `torch.cuda.max_memory_allocated() / 1024**3` in appropriate place in the `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original llama 1b config\n",
    "config = {\n",
    "    \"vocab_size\": 128256,\n",
    "    \"hidden_size\": 2048,\n",
    "    \"intermediate_size\": 8192,\n",
    "    \"num_hidden_layers\": 16,\n",
    "    \"num_attention_heads\": 32,\n",
    "    \"num_key_value_heads\": 8,\n",
    "    \"max_position_embeddings\": 131072,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_theta\": 500000.0,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"hidden_dropout\": 0.0,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"mlp_bias\": False,\n",
    "    \"attention_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Configurable Llama Training (Single GPU)\n",
      "==================================================\n",
      "Using device: cuda:0\n",
      "Loading tokenizer...\n",
      "Creating model with random initialization...\n",
      "üìä Actual model parameters: 1,498,482,688\n",
      "üéØ Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000 examples\n",
      "Tokenizing dataset...\n",
      "Train examples: 900\n",
      "Eval examples: 100\n",
      "\n",
      "üîß Training Configuration:\n",
      "   ‚Ä¢ Batch Size: 1\n",
      "   ‚Ä¢ Max Sequence Length: 256\n",
      "   ‚Ä¢ Learning Rate: 0.0005\n",
      "   ‚Ä¢ Mixed Precision: True\n",
      "\n",
      "üèãÔ∏è Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.958200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stretch] Use PyTorch Profiler to profile training and inference of the above config [20 min]\n",
    "\n",
    "Generate a trace to analyze the workloads for training and inference of the same model. See documentation here: https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Capital of Germany is Berlin\\nBerlin is the capital and largest city of Germany, located in the central part of the country'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Activation Checkpointing [20 min]\n",
    "\n",
    "Add activation checkpointing after every transformer layer to the `train` function and analyze its effect on peak memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Accumulation [20 min]\n",
    "\n",
    "Currently we trained with a batch size of 1, increase this (global) batch size to a larger value and make it run on a single GPU by passing gradient accumulation to TrainingArgs in the `train` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
